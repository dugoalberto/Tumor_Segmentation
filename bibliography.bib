@article{HAVAEI201718,
title = {Brain tumor segmentation with Deep Neural Networks},
journal = {Medical Image Analysis},
volume = {35},
pages = {18-31},
year = {2017},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2016.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1361841516300330},
author = {Mohammad Havaei and Axel Davy and David Warde-Farley and Antoine Biard and Aaron Courville and Yoshua Bengio and Chris Pal and Pierre-Marc Jodoin and Hugo Larochelle},
keywords = {Brain tumor segmentation, Deep neural networks, Convolutional neural networks, Cascaded convolutional neural networks},
abstract = {In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we’ve found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data. We present a novel CNN architecture which differs from those traditionally used in computer vision. Our CNN exploits both local features as well as more global contextual features simultaneously. Also, different from most traditional uses of CNNs, our networks use a final layer that is a convolutional implementation of a fully connected layer which allows a 40 fold speed up. We also describe a 2-phase training procedure that allows us to tackle difficulties related to the imbalance of tumor labels. Finally, we explore a cascade architecture in which the output of a basic CNN is treated as an additional source of information for a subsequent CNN. Results reported on the 2013 BRATS test data-set reveal that our architecture improves over the currently published state-of-the-art while being over 30 times faster.}
}
@misc{cao2021swinunetunetlikepuretransformer,
      title={Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation}, 
      author={Hu Cao and Yueyue Wang and Joy Chen and Dongsheng Jiang and Xiaopeng Zhang and Qi Tian and Manning Wang},
      year={2021},
      eprint={2105.05537},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2105.05537}, 
}
@misc{çiçek20163dunetlearningdense,
      title={3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation}, 
      author={Özgün Çiçek and Ahmed Abdulkadir and Soeren S. Lienkamp and Thomas Brox and Olaf Ronneberger},
      year={2016},
      eprint={1606.06650},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1606.06650}, 
}
@misc{isensee2020nnunetbraintumorsegmentation,
      title={nnU-Net for Brain Tumor Segmentation}, 
      author={Fabian Isensee and Paul F. Jaeger and Peter M. Full and Philipp Vollmuth and Klaus H. Maier-Hein},
      year={2020},
      eprint={2011.00848},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2011.00848}, 
}
@article{GORDILLO20131426,
title = {State of the art survey on MRI brain tumor segmentation},
journal = {Magnetic Resonance Imaging},
volume = {31},
number = {8},
pages = {1426-1438},
year = {2013},
issn = {0730-725X},
doi = {https://doi.org/10.1016/j.mri.2013.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0730725X13001872},
author = {Nelly Gordillo and Eduard Montseny and Pilar Sobrevilla},
keywords = {Segmentation, MRI, Brain tumor},
abstract = {Brain tumor segmentation consists of separating the different tumor tissues (solid or active tumor, edema, and necrosis) from normal brain tissues: gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF). In brain tumor studies, the existence of abnormal tissues may be easily detectable most of the time. However, accurate and reproducible segmentation and characterization of abnormalities are not straightforward. In the past, many researchers in the field of medical imaging and soft computing have made significant survey in the field of brain tumor segmentation. Both semiautomatic and fully automatic methods have been proposed. Clinical acceptance of segmentation techniques has depended on the simplicity of the segmentation, and the degree of user supervision. Interactive or semiautomatic methods are likely to remain dominant in practice for some time, especially in these applications where erroneous interpretations are unacceptable. This article presents an overview of the most relevant brain tumor segmentation methods, conducted after the acquisition of the image. Given the advantages of magnetic resonance imaging over other diagnostic imaging, this survey is focused on MRI brain tumor segmentation. Semiautomatic and fully automatic techniques are emphasized.}
}

@misc{CBTRUSTumors2022,
    author = {CBTRUS - Central Brain Tumor Registry of the United States},
    title = {CBTRUS Fact Sheet},
    url = {https://cbtrus.org/cbtrus-fact-sheet/},
    accessed = {31/03/2025}
}

@misc{BraTSChallenge,
    author={Perelman School of Medicine - University of Pennsylvania},
    title={Brain Tumor Segmentatation (BraTS) Challenge},
    url = {https://www.med.upenn.edu/cbica/brats/},
    accessed = {31/05/2024}

}

@misc{BraTSdataset,
    author={Kaggle},
    title={Kaggle BraTS2020 Dataset},
    url = {https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation},
    accessed = {04/04/2025}

}

@InProceedings{He_2015_ICCV,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
}

@inproceedings{dong2017automatic,
  title={Automatic brain tumor detection and segmentation using U-Net based fully convolutional networks},
  author={Dong, Hao and Yang, Guang and Liu, Fangde and Mo, Yuanhan and Guo, Yike},
  booktitle={Medical Image Understanding and Analysis: 21st Annual Conference, MIUA 2017, Edinburgh, UK, July 11--13, 2017, Proceedings 21},
  pages={506--517},
  year={2017},
  organization={Springer}
}

@inproceedings{bukhari2021e1d3,
  title={E1D3 U-Net for brain tumor segmentation: Submission to the RSNA-ASNR-MICCAI BraTS 2021 challenge},
  author={Bukhari, Syed Talha and Mohy-ud-Din, Hassan},
  booktitle={International MICCAI Brainlesion Workshop},
  pages={276--288},
  year={2021},
  organization={Springer}
}

@inproceedings{myronenko2019robust,
  title={Robust semantic segmentation of brain tumor regions from 3D MRIs},
  author={Myronenko, Andriy and Hatamizadeh, Ali},
  booktitle={International MICCAI Brainlesion Workshop},
  pages={82--89},
  year={2019},
  organization={Springer}
}

@article{Ahmad2025liu,
author = {Ahmad, Jameel and Ksibi, Amel and Alsenan, Shrooq and Arshad, Arfan and Raza, Rehan and Shaikh, Zaffar},
year = {2025},
month = {03},
pages = {e2787},
title = {LIU-NET: lightweight Inception U-Net for efficient brain tumor segmentation from multimodal 3D MRI images},
volume = {11},
journal = {PeerJ Computer Science},
doi = {10.7717/peerj-cs.2787}
}

@ARTICLE{Liu2024BTIS,
  author={Liu, Li and Xia, Kaijian},
  journal={IEEE Access}, 
  title={BTIS-Net: Efficient 3D U-Net for Brain Tumor Image Segmentation}, 
  year={2024},
  volume={12},
  number={},
  pages={133392-133405},
  abstract={Brain tumor segmentation techniques are essential for the precise delineation of tumors and normal brain tissues which is essential for the guidance of surgical intervention and clinical decisions. However, for resource-constrained clinical environments, more efficient and lightweight segmentation models are needed so that they can be applied in real-time for surgical navigation and clinical decision-making. To tackle this issue, the proposed study introduces a very effective 3D U-Net model that is specifically designed for brain tumor image segmentation. This study presents the primary contributions as follows: 3D depth separable convolution is introduced to decrease the number of model training parameters, hence enhancing the overall efficiency of the model. The dilated dense residual block is designed to expand the sensory field, allowing the network to grasp a broader range of features and structures within the input data. As a result, the model’s performance and generalization ability to handle complex tasks are improved. The integration of the confusion area segmentation module enhances the model’s capability to discern intricate image details and edges, thereby augmenting the overall segmentation efficacy. Evaluation of the proposed BTIS-Net involves experimentation on two widely recognized datasets, namely BraTS 2019 and BraTS 2021. The Dice similarity coefficient, Positive predictive value, and Sensitivity exhibit average improvements of 5.68, 5.38, and 2.14, respectively. Additionally, the Hausdorff distance is reduced by an average of 2.71. The experimental results validate the efficient segmentation performance of the BTIS-Net model, showcasing exceptional outcomes even under resource constraints.},
  keywords={Image segmentation;Tumors;Brain modeling;Three-dimensional displays;Convolutional neural networks;Magnetic resonance imaging;Solid modeling;Brain cancer;3D U-Net;3D depth separable convolution;brain tumor image segmentation;confusion area segmentation;dilated dense residual block},
  doi={10.1109/ACCESS.2024.3460797},
  ISSN={2169-3536},
  month={},}

  @article{An2024DWKD,
title = {Dynamic weighted knowledge distillation for brain tumor segmentation},
journal = {Pattern Recognition},
volume = {155},
pages = {110731},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110731},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004825},
author = {Dianlong An and Panpan Liu and Yan Feng and Pengju Ding and Weifeng Zhou and Bin Yu},
keywords = {Brain tumor segmentation, MRI, Static knowledge distillation, Dynamic weighted knowledge distillation, Interpretability},
abstract = {Automatic 3D MRI brain tumor segmentation holds a crucial position in the field of medical image analysis, contributing significantly to the clinical diagnosis and treatment of brain tumors. However, traditional 3D brain tumor segmentation methods often entail extensive parameters and computational demands, posing substantial challenges in model training and deployment. To overcome these challenges, this paper introduces a brain tumor segmentation framework based on knowledge distillation. This framework includes training a lightweight network by extracting knowledge from a well-established brain tumor segmentation network. Firstly, this framework replaces the conventional static knowledge distillation (SKD) with the proposed dynamic weighted knowledge distillation (DWKD). DWKD dynamically adjusts the distillation loss weights for each pixel based on the learning state of the student network. Secondly, to enhance the student network's generalization capability, this paper customizes a loss function for DWKD, known as regularized cross-entropy (RCE). RCE introduces controlled noise into the model, enhancing its robustness and diminishing the risk of overfitting. This controlled injection of noise aids in fortifying the model's robustness. Lastly, Empirical validation of the proposed methodology is conducted using two distinct backbone networks, namely Attention U-Net and Residual U-Net. Rigorous experimentation is executed across the BraTS 2019, BraTS 2020, and BraTS 2021 datasets. Experimental results demonstrate that DWKD exhibits significant advantages over SKD in enhancing the segmentation performance of the student network. Furthermore, when dealing with limited training data, the RCE method can further improve the student network's segmentation performance. Additionally, this paper quantitatively analyzes the number of concept detectors identified in network dissection. It assesses the impact of DWKD on model interpretability and finds that compared to SKD, DWKD can more effectively enhance model interpretability. The source code is available at https://github.com/YuBinLab-QUST/DWKD/.}
}

@Article{Gutierrez2024Brain,
AUTHOR = {Hernandez-Gutierrez, Fernando Daniel and Avina-Bravo, Eli Gabriel and Zambrano-Gutierrez, Daniel F. and Almanza-Conejo, Oscar and Ibarra-Manzano, Mario Alberto and Ruiz-Pinales, Jose and Ovalle-Magallanes, Emmanuel and Avina-Cervantes, Juan Gabriel},
TITLE = {Brain Tumor Segmentation from Optimal MRI Slices Using a Lightweight U-Net},
JOURNAL = {Technologies},
VOLUME = {12},
YEAR = {2024},
NUMBER = {10},
ARTICLE-NUMBER = {183},
URL = {https://www.mdpi.com/2227-7080/12/10/183},
ISSN = {2227-7080},
ABSTRACT = {The timely detection and accurate localization of brain tumors is crucial in preserving people’s quality of life. Thankfully, intelligent computational systems have proven invaluable in addressing these challenges. In particular, the UNET model can extract essential pixel-level features to automatically identify the tumor’s location. However, known deep learning-based works usually directly feed the 3D volume into the model, which causes excessive computational complexity. This paper presents an approach to boost the UNET network, reducing computational workload while maintaining superior efficiency in locating brain tumors. This concept could benefit portable or embedded recognition systems with limited resources for operating in real time. This enhancement involves an automatic slice selection from the MRI T2 modality volumetric images containing the most relevant tumor information and implementing an adaptive learning rate to avoid local minima. Compared with the original model (7.7 M parameters), the proposed UNET model uses only 2 M parameters and was tested on the BraTS 2017, 2020, and 2021 datasets. Notably, the BraTS2021 dataset provided outstanding binary metric results: 0.7807 for the Intersection Over the Union (IoU), 0.860 for the Dice Similarity Coefficient (DSC), 0.656 for the Sensitivity, and 0.9964 for the Specificity compared to vanilla UNET.},
DOI = {10.3390/technologies12100183}
}




